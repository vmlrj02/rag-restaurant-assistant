# ğŸ• Restaurant Review RAG Assistant (LangChain + Ollama + Chroma)

A **Retrieval-Augmented Generation (RAG)** assistant built using **LangChain**, **Ollama**, and **ChromaDB**.  
It analyzes restaurant reviews and answers questions using both semantic search and LLM reasoning.

---

## ğŸš€ Features

- Offline LLM using **Ollama**
- **mxbai-embed-large** for embeddings
- **ChromaDB** for vector search
- Fully local RAG pipeline
- Interactive CLI chatbot
- Clean project structure

---

## ğŸ“ Project Structure

```
AIAgent/
â”‚â”€â”€ main.py                  # Chat/chat loop
â”‚â”€â”€ vector.py                # Vector store creation + retriever
â”‚â”€â”€ realistic_restaurant_reviews.csv
â”‚â”€â”€ README.md
â”‚â”€â”€ venv/
â”‚â”€â”€ .gitignore
```

---

## â–¶ï¸ Setup Instructions

### 1ï¸âƒ£ Install Ollama  
Download: https://ollama.com/download

Verify:
```bash
ollama --version
```

### 2ï¸âƒ£ Install required LLM + embedding model

```bash
ollama pull llama3.1
ollama pull mxbai-embed-large
```

---

## 3ï¸âƒ£ Create virtual environment

```bash
python3 -m venv venv
source venv/bin/activate
```

---

## 4ï¸âƒ£ Install dependencies

```bash
pip install langchain langchain-ollama langchain-chroma chromadb pandas
```

---

## 5ï¸âƒ£ Build vector database

```bash
python3 vector.py
```

---

## 6ï¸âƒ£ Run chatbot

```bash
python3 main.py
```

Input example:
```
What is the best pizza in town?
```

---

## ğŸ§  How RAG Works

1. User question  
2. Retriever finds top relevant reviews  
3. LLM (via Ollama) receives:  
   - reviews  
   - user question  
4. LLM generates a final answer  

---

## ğŸ“Œ Future Enhancements
- Add FastAPI API endpoint  
- Add UI (Streamlit / Gradio)  
- Add sentiment analysis  
- Add vector store evaluation  

---

## ğŸ† Good For
Portfolio projects for:
- AI Engineers  
- LLM Developers  
- Data Scientists  
- NLP/RAG Engineers
